---
title: "Reproducible online supplementary material"
author: "Rachel Heyard et al. (2021)"
output: 
  bookdown::html_document2:
    fig_cap: TRUE
    theme: paper
    highlight: null
    mathjax: null
    self_contained: true
    code_folding: hide
---

```{r header, include=FALSE, message = FALSE, warning=FALSE}
knitr::opts_chunk$set(fig.showtext = TRUE)
# kableExtra for kable-tables
library(kableExtra)

# Use here("...") with paths relative to the project root
# to address files
library(here)
library(stringr)
# Some formatting function(s)
pretty <- function(x) prettyNum(x, big.mark = "'")
round_2 <- function(x) round(x, 2)
round_3 <- function(x) round(x, 3)

options(dplyr.summarise.inform = FALSE)

options(knitr.kable.NA = '')
```

This online supplement reproduces all the Figures and Tables presented in the publication "Rethinking the Funding Line at the Swiss National Science Foundation: Bayesian Ranking and Lottery". The methodology of the expected rank (ER) for grant peer review is fully implemented in the `R` package `{ERforResearch}` downloadable from [github](https://github.com/snsf-data/ERforResearch) with [documentation](https://snsf-data.github.io/ERforResearch/). To install the package, the following code can be used:
```{r, message = FALSE, class.source = "fold-show", warning=FALSE}
# Install ERforResearch from github:
# devtools::install_github("snsf-data/ERforResearch")
library(ERforResearch)
```



```{r helper functions, class.source = "fold-show"}
get_icc_from_matrix <- function(individual_votes) {
  (individual_votes %>% 
     select(-proposal) %>% 
     mutate_all(function(x) ERforResearch:::get_num_grade_snsf(x)) %>% 
     as.data.frame() %>% 
     psych::ICC(x = ., missing = FALSE))$results %>% 
    filter(type == "ICC3") %>% # A fixed set of k judges rates each target
    mutate(icc = paste0(round(ICC, 2), " (",
                        round(`lower bound`, 2), "; ", 
                        round(`upper bound`, 2), ")")) %>% 
    pull(icc) %>% 
    return()
}

get_grades_plot <- function(long_individual_votes, individual_votes_mat = NULL,
                            x_min = NULL, x_max = NULL, title = "",
                            jitter_h = .02, jitter_w = .025,
                            jitter_alpha = .5){
  if (is.null(x_min)){
    x_min <- long_individual_votes %>% pull(num_grade) %>% min
    x_min <- ifelse(x_min > 1, x_min - 1.2, x_min -.2)
  }
  if (is.null(x_max)){
    x_max <- long_individual_votes %>% pull(num_grade) %>% max
    x_max <- ifelse(x_max < 6, x_max + 1.2, x_max +.2)
  }
  if (!is.null(individual_votes_mat)){
    icc <- get_icc_from_matrix(individual_votes_mat)
    title <- paste0(ifelse(title == "", "ICC = ",
                           paste0(title, ": ICC = ")), icc)
  }
  plot_data <- long_individual_votes %>% 
    group_by(proposal) %>% 
    mutate(avg = mean(num_grade, na.rm = TRUE)) %>% 
    ungroup() 
  plot_data <- plot_data %>% 
    select(proposal, avg) %>% 
    distinct() %>% 
    arrange(avg) %>% 
    mutate(order = 1:n()) %>% 
    select(-avg) %>% 
    left_join(plot_data, by = "proposal")
  plot_data %>% 
    ggplot(aes(x = num_grade, y = order)) + 
    geom_jitter(aes(color = "Individual Votes"), height = jitter_h, 
                width = jitter_w, alpha = jitter_alpha, size = 1) +
    geom_point(aes(x = avg, color = "Average"), size = 1.5) + 
    theme_minimal() +
    scale_y_continuous(breaks = unique(plot_data$order),
                       labels = 
                         as.numeric(str_extract_all(unique(plot_data$proposal),
                                                    "[0-9]+"))) +
    labs(x = "Assessor Vote", y = "Ordered proposal", title = title) +
    lims(x = c(x_min, x_max)) +
    scale_color_manual(name = " ", 
                       values = c("Average"="#FF6F6F",
                                  "Individual Votes" = "#848383")) +
    theme(axis.text.y = element_blank(),
          legend.position = c(0.5, .5),
          axis.title.y = element_blank(),
          panel.grid.major.y = element_line(size = .1))
}

get_agreement_plot <- function(rankings, rankings_ordinal, title, ymin = -.5,
                               ymax = .5, pt_alpha = .8, pt_size = 1){
  difference <- rankings %>%  
    select(id_application, er) %>% 
    rename(er1 = er) %>% 
    left_join(rankings_ordinal %>% 
                select(id_application, er) %>% 
                rename(er2 = er), 
              by = "id_application") %>% 
    mutate(difference = er1 - er2,
           avg = (er1 + er2)/2) %>% 
    select(id_application, difference, avg)
  
  d <- difference %>% 
    summarise(mean(difference)) %>% pull
  sd <- difference %>% 
    summarise(sd(difference)) %>% pull
  difference %>% 
    ggplot(aes(x = avg, y = difference)) + 
    geom_point(alpha = pt_alpha, size = pt_size) + 
    geom_hline(yintercept = d, color = "gray") + 
    geom_hline(yintercept = d - 1.96 * sd, color = "darkred", lty = "dotted") + 
    geom_hline(yintercept = d + 1.96 * sd, color = "darkred", lty = "dotted") + 
    ylim(ymin, ymax) + 
    labs(y = "Difference in ER", x = "Average ER of both approaches",
         title = title) +
    snf.plot::snsf_theme 
}

percentile_loss <- function(true_rank, estimated_rank, perc = .3, p = 2,
                            q = 2, c = 1){
  I <- length(true_rank)
  true_percentile <- true_rank / (I + 1)
  estimated_percentile <- estimated_rank / (I + 1)
  AB <- (true_percentile > perc) & (estimated_percentile < perc)
  BA <- (true_percentile < perc) & (estimated_percentile > perc)
  
  loss <- (sum(abs(perc - estimated_percentile)^p * AB +
                 c * abs(estimated_percentile - perc)^q * BA)) / I
  return(loss)
}
```


The main paper presents two major case studies. The methodology of the ER is applied to give funding recommendations for two funding instruments at the Swiss National Science Foundation(SNSF): the _Postdoc.Mobility_ fellowship for early career researchers and the SNSF _Project Funding_ scheme. The data for both case studies is available on [Zenodo](https://zenodo.org/record/4531160#.YCQFZehKh3g) and can accessed directly through `R` using the following code chunk:


```{r load-data, message = FALSE, class.source = "fold-show", warning=FALSE}
# Load the data from Zenodo:
path_to_xlsx <- "https://zenodo.org/record/4531160/files/individual_votes.xlsx"

hss_s_mat <- openxlsx::read.xlsx(xlsxFile = path_to_xlsx, sheet = "pm_hsss") 
hss_s <- hss_s_mat %>% 
  get_right_data_format(prefix_voter = "voter")

hss_h_mat <- openxlsx::read.xlsx(xlsxFile = path_to_xlsx, sheet = "pm_hssh") 
hss_h <- hss_h_mat %>% 
  get_right_data_format(prefix_voter = "voter")

ls_b_mat <- openxlsx::read.xlsx(xlsxFile = path_to_xlsx, sheet = "pm_lsb") 
ls_b <- ls_b_mat %>% 
  get_right_data_format(prefix_voter = "voter")

ls_m_mat <- openxlsx::read.xlsx(xlsxFile = path_to_xlsx, sheet = "pm_lsm") 
ls_m <- ls_m_mat %>% 
  get_right_data_format(prefix_voter = "voter")

stem_mat <- openxlsx::read.xlsx(xlsxFile = path_to_xlsx, sheet = "pm_stem")
stem <- stem_mat %>% 
  get_right_data_format(prefix_voter = "voter")

mint_section1_mat <- 
  openxlsx::read.xlsx(xlsxFile = path_to_xlsx, sheet = "mint_section1") 
mint_section1 <- mint_section1_mat %>% 
  get_right_data_format(prefix_voter = "voter") %>% 
  mutate(section = "one")

mint_section2_mat <- 
  openxlsx::read.xlsx(xlsxFile = path_to_xlsx, sheet = "mint_section2") 
mint_section2 <- mint_section2_mat %>% 
  get_right_data_format(prefix_voter = "voter") %>% 
  mutate(section = "two")

mint_section3_mat <- 
  openxlsx::read.xlsx(xlsxFile = path_to_xlsx, sheet = "mint_section3")
mint_section3 <- mint_section3_mat %>% 
  get_right_data_format(prefix_voter = "voter") %>% 
  mutate(section = "three")

mint_section4_mat <- 
  openxlsx::read.xlsx(xlsxFile = path_to_xlsx, sheet = "mint_section4") 
mint_section4 <- mint_section4_mat %>% 
  get_right_data_format(prefix_voter = "voter") %>% 
  mutate(section = "four")

# Note that we want to be able to differentiate the proposals in the different
# mint sections 
mint_sections <- mint_section1 %>% 
  mutate(proposal = paste0(proposal, "_1")) %>% 
  bind_rows(mint_section2%>% 
              mutate(proposal = paste0(proposal, "_2"))) %>% 
  bind_rows(mint_section3%>% 
              mutate(proposal = paste0(proposal, "_3"))) %>% 
  bind_rows(mint_section4%>% 
              mutate(proposal = paste0(proposal, "_4")))

##### How many projects can still be funded? ####
how_many_fundable <- c(7, 4, 7, 8, 6)
names(how_many_fundable) <- c("hss_s", "hss_h", "ls_m", "ls_b", "stem")

```


# Postdoc.Mobility funding scheme 

The fist case study focuses on the evaluation of the [Postdoc.Mobility (PM)](http://www.snf.ch/en/funding/careers/postdoc-mobility/Pages/default.aspx) Call of February 1st 2020. Junior researchers who have recently defended their PhD and wish to pursue an academic career can apply for a PM fellowship, which will allow them to work in a research group abroad for two years. The researchers submit their research proposal to the SNSF, which is then evaluated by two referees who score the proposal on a six-point scale (1 being poor quality and 6 being outstanding quality). Each proposal is evaluated by one of the following panels: Humanities; Social Sciences; Science, Technology, Engineering and Mathematics (STEM); Medicine; and Biology. As described in the main paper, the dataset on the PM panels contains the evaluation of the voters on the proposoals which were triaged into the 'panel discussion' group ($e.g.$ not directly accepted, nor rejected). Each panel has their own available funding, so that five different funding lines are drawn. The proposals are discussed in the respective panel, where each member votes on a score for each of the proposals, unless they have a conflict of interest or another reason to be absent during the voting. Table \@ref(tab:number-applications-pm) summarizes the number of proposals in the different 'panel discussion' groups together with the number of proposals that can still be funded and the size of the panel. 

```{r number-applications-pm, echo=FALSE}
how_many_fundable2 <- how_many_fundable
names(how_many_fundable2) <- c("HSS-S", "HSS-H", "LS-M", "LS-B", "STEM")

how_many_submitted2 <- c(38, 23, 35, 35, 50)
names(how_many_submitted2) <- c("HSS-S", "HSS-H", "LS-M", "LS-B", "STEM")

all_data_pm <- 
  hss_s %>% 
  mutate(panel = "HSS-S", 
         panel_name = "hss_s",
         application_number = paste0(proposal, "_A")) %>% 
  bind_rows(hss_h %>% 
              mutate(panel = "HSS-H",
                     panel_name = "hss_h",
                     application_number = paste0(proposal, "_B"))) %>% 
  bind_rows(ls_b %>% 
              mutate(panel = "LS-B",
                     panel_name = "ls_b",
                     application_number = paste0(proposal, "_C"))) %>% 
  bind_rows(ls_m %>% 
              mutate(panel = "LS-M",
                     panel_name = "ls_m",
                     application_number = paste0(proposal, "_D"))) %>% 
  bind_rows(stem %>% 
              mutate(panel = "STEM",
                     panel_name = "stem",
                     application_number = paste0(proposal, "_E"))) 

all_data_pm %>% 
  mutate(funds = how_many_fundable2[panel], sub = how_many_submitted2[panel]
         ) %>% 
  group_by(panel, funds, sub
           ) %>%
  summarise(discussed = n_distinct(application_number),
            panel_members = n_distinct(voter)) %>% 
  mutate(panel = case_when(panel == "HSS-S" ~ "Social Sciences",
                           panel == "HSS-H" ~ "Humanities",
                           panel == "LS-B" ~ "Biology",
                           panel == "LS-M" ~ "Medicine",
                           panel == "STEM" ~ "STEM")) %>%
  select(panel, panel_members, sub, discussed, funds) %>% 
  rename("N. of fundable proposals" = funds,
         "N. of proposals discussed" = discussed,
         "Total N. of submissions" = sub,
         "N. of panel members" = panel_members,
         " " = panel) %>% 
  kableExtra::kable(booktabs = TRUE,
               caption = "Characteristics of panels and submissions for the 
               February 2020 call for junior fellowships at the Swiss National
               Science Foundation. \\label{tab:number_applications_pm}") %>% 
  kable_material(c("striped", "hover"))

```

For each of the five panels, Figure \@ref(fig:all-grades-pm2020) presents the votes of the panel members to each of the proposals discussed. The proposals are represented on the y-axis, and ordered depending on the average of the individual votes they got (red dots). 
We see a large variability in the votes.
Note that we only consider the middle block of proposals that were neither truly exceptional nor of very low quality
and that the variability of the votes per proposals tends to be larger in the middle block than for proposals a with very low/high average score.
Some of the members of the STEM panel still assessed a few proposals with a 2, which is the second lowest grade; not surprisingly the STEM panel has the lowest intraclass correlation (ICC), representing the assessor reliability.
In all 5 panels, the assessor reliability can be classified as either poor or fair.

```{r grade-distribution-plots, fig.height=8, fig.width=9, fig.align="center", warning=FALSE, message=FALSE, fig.cap = "<span style='color: #d3d3d3;'>The individual votes given to all proposals by all panel members. The red dots represent the averages of those individual votes for each proposal. The intra-class correlation coefficients together with their 95\\% confidence intervals of the different panels can be found in the titles of the subfigures. In all panels, the assessor reliability can be classified as poor to fair.</span>\\label{fig:all-grades-pm2020}"}

leg <- cowplot::get_legend(get_grades_plot(hss_h))
gridExtra::grid.arrange(get_grades_plot(hss_h, hss_h_mat, x_min = 1.8, 
                                        title = "Humanities") +
                          theme(legend.position = "none"),
                        get_grades_plot(hss_s, hss_s_mat, x_min = 1.8,
                                        title = "Social Sciences") + 
                          theme(legend.position = "none"),
                        get_grades_plot(ls_b, ls_b_mat, x_min = 1.8,
                                        title = "Biology")+ 
                          theme(legend.position = "none"),
                        get_grades_plot(ls_m, ls_m_mat, x_min = 1.8,
                                        title = "Medicine")+ 
                          theme(legend.position = "none"),
                        get_grades_plot(stem, stem_mat, x_min = 1.8, 
                                        title = "STEM")+ 
                          theme(legend.position = "none"), 
                        leg)
```

Then, the Bayesian hierarchical models will be fitted for each of the five panels individually and the expected ranks will be retrieved using the MCMC samples from the latter models.

```{r er-results-pm2020, warning=FALSE, message=FALSE,  class.source = "fold-show", cache = TRUE}

##### The model for JAGS ####
# we will use the default

##### Get the ER results: ####
n.iter <- 10000
n.burnin <- 5000
n.chains <- 2
seed <- 1991
mcmc_hss_s_object <- 
  get_mcmc_samples(data = hss_s,
                   id_application = "proposal",
                   id_voter = "voter", 
                   grade_variable = "num_grade",
                   n_chains = n.chains, n_iter = n.iter,
                   n_burnin = n.burnin,
                   other_variables = "nu",
                   seed = seed, quiet = TRUE, dont_bind = TRUE)
mcmc_hss_s <- mcmc(do.call(rbind, mcmc_hss_s_object))

mcmc_hss_h_object <- 
  get_mcmc_samples(data = hss_h,
                   id_application = "proposal",
                   id_voter = "voter", 
                   grade_variable = "num_grade",
                   n_chains = n.chains, n_iter = n.iter,
                   n_burnin = n.burnin,
                   other_variables = "nu",
                   seed = seed, quiet = TRUE, dont_bind = TRUE)
mcmc_hss_h <- mcmc(do.call(rbind, mcmc_hss_h_object))

mcmc_ls_b_object <- 
  get_mcmc_samples(data = ls_b,
                   id_application = "proposal",
                   id_voter = "voter", 
                   grade_variable = "num_grade",
                   n_chains = n.chains, n_iter = n.iter,
                   n_burnin = n.burnin,
                   other_variables = "nu",
                   seed = seed, quiet = TRUE, dont_bind = TRUE)
mcmc_ls_b <- mcmc(do.call(rbind, mcmc_ls_b_object))

mcmc_ls_m_object <- 
  get_mcmc_samples(data = ls_m,
                   id_application = "proposal",
                   id_voter = "voter", 
                   grade_variable = "num_grade",
                   n_chains = n.chains, n_iter = n.iter,
                   n_burnin = n.burnin,
                   other_variables = "nu",
                   seed = seed, quiet = TRUE, dont_bind = TRUE)
mcmc_ls_m <- mcmc(do.call(rbind, mcmc_ls_m_object))

mcmc_stem_object <- 
  get_mcmc_samples(data = stem,
                   id_application = "proposal",
                   id_voter = "voter", 
                   grade_variable = "num_grade",
                   n_chains = n.chains, n_iter = n.iter,
                   n_burnin = n.burnin,
                   other_variables = "nu",
                   seed = seed, quiet = TRUE, dont_bind = TRUE)

mcmc_stem <- mcmc(do.call(rbind, mcmc_stem_object))


# Computation of the ER and PCER:
ranks_hss_s <- get_er_from_jags(data = hss_s,
                                id_application = "proposal",
                                id_voter = "voter", 
                                grade_variable = "num_grade",
                                mcmc_samples = mcmc_hss_s)

ranks_hss_h <- get_er_from_jags(data = hss_h,
                                id_application = "proposal",
                                id_voter = "voter", 
                                grade_variable = "num_grade",
                                mcmc_samples = mcmc_hss_h)

ranks_ls_b <- get_er_from_jags(data = ls_b,
                               id_application = "proposal",
                               id_voter = "voter", 
                               grade_variable = "num_grade",
                               mcmc_samples = mcmc_ls_b)

ranks_ls_m <- get_er_from_jags(data = ls_m,
                               id_application = "proposal",
                               id_voter = "voter", 
                               grade_variable = "num_grade",
                               mcmc_samples = mcmc_ls_m)

ranks_stem <- get_er_from_jags(data = stem,
                               id_application = "proposal",
                               id_voter = "voter", 
                               grade_variable = "num_grade",
                               mcmc_samples = mcmc_stem)


```

Figure \@ref(fig:ER-plot-pm2020) presents the different stages of ranking of the proposals for all panels together with the rankability. The first column of points in the figure represents ranking based on the simple averages (Fixed). Then the proposals are ranked based on the posterior means of $\theta_i$ (Posterior Mean) and finally, the last column of points the expected rank. The rankability differs from panel to panel. A naive funding line as represented by the color code in the figure is defined by simply funding the $x$ best ranked proposals, where $x$ can be retrieved from Table \@ref(tab:number-applications-pm). Lower rankability suggests that more proposals are clustered and therefore not clearly differentiatable with their competitors. However, rankability is not an indication on how easy discrimination between funded and not funded proposals can be done (see the Humanities panel, with a relatively low rankability, but two clearly differentiated groups). 

```{r ER-plot-pm2020, fig.height=8, fig.width=9, warning=FALSE, message=FALSE, fig.cap = "<span style='color: #d3d3d3;'>PM fellowship proposals discussed in the different panels ranked using the average overall score (Fixed), the posterior means of the parameter $\\theta_i$ and the ER. The color code shows which proposals would be funded, if the SNSF simply funds the $x$ best proposals based on the ER, where $x$ is panel-specific and shown in Table \\@ref(tab:number-applications-pm). Points representing tied proposals are on top of each other, but noticable as such because of the numbers.</span>  \\label{fig:ER_pm_2020}",  fig.align="center"}
##### Plot the results ####
plot_hss_s <-
  plotting_er_results(ranks_hss_s, result_show = TRUE, title = "Social Science",
                      id_application = "id_application",
                      pt_size = .5, line_size = .2, draw_funding_line = FALSE, 
                      line_type_fl = "longdash", color_fl = "darkgray", grep_size = 3,
                      how_many_fundable = how_many_fundable["hss_s"])

plot_hss_h <- 
  plotting_er_results(ranks_hss_s, result_show = TRUE, title = "Humanities",
                      id_application = "id_application",
                      pt_size = .5, line_size = .2, draw_funding_line = FALSE, 
                      line_type_fl = "longdash", color_fl = "darkgray", grep_size = 3,
                      how_many_fundable = how_many_fundable["hss_h"]) 

plot_ls_b <-
  plotting_er_results(ranks_hss_s, result_show = TRUE, title = "Biology",
                      id_application = "id_application",
                      pt_size = .5, line_size = .2, draw_funding_line = FALSE, 
                      line_type_fl = "longdash", color_fl = "darkgray", grep_size = 3,
                      how_many_fundable = how_many_fundable["ls_b"]) 

plot_ls_m <- 
   plotting_er_results(ranks_hss_s, result_show = TRUE, title = "Medicine",
                      id_application = "id_application",
                      pt_size = .5, line_size = .2, draw_funding_line = FALSE, 
                      line_type_fl = "longdash", color_fl = "darkgray", grep_size = 3,
                      how_many_fundable = how_many_fundable["ls_m"]) 

plot_stem <-  
  plotting_er_results(ranks_hss_s, result_show = TRUE, title = "STEM",
                      id_application = "id_application",
                      pt_size = .5, line_size = .2, draw_funding_line = FALSE, 
                      line_type_fl = "longdash", color_fl = "darkgray", grep_size = 3,
                      how_many_fundable = how_many_fundable["stem"]) 

leg <- cowplot::get_legend(plot_hss_h)
gridExtra::grid.arrange(plot_hss_h +  
                          theme(legend.position = "none",
                                axis.title.x = element_blank(),
                                axis.text.x = element_blank(), 
                                axis.title.y = element_text(size = 7),
                                title = element_text(size = 7)),
                        plot_hss_s + 
                          theme(legend.position = "none",
                                axis.title.y = element_blank(),
                                axis.title.x = element_blank(),
                                axis.text.x = element_blank(),
                                title = element_text(size = 7)),
                        plot_ls_b + 
                          theme(legend.position = "none",
                                axis.title.x = element_blank(),
                                axis.title.y = element_blank(),
                                title = element_text(size = 7)),
                        plot_ls_m +
                          theme(legend.position = "none",
                                axis.title.x = element_blank(),
                                axis.title.y = element_text(size = 7),
                                title = element_text(size = 7)),
                        plot_stem + 
                          theme(legend.position = "none",
                                axis.title.y = element_blank(),
                                title = element_text(size = 7)),
                        leg,
                        ncol = 3)
```

To fully account for the uncertainty, we plot the same ER of the proposals together with their 50\% credible intervals in Figure \@ref(fig:rank-credible-intervals-pm2020). The methodology then recommends the following decisions: 

* Humanities panel: The four best ranked proposals are funded, the remaining seven are rejected, no random selection element.
* Social Sciences: The six best ranked proposals are funded, the ten worst ranked proposals are rejected. One proposal is randomly selected for funding among the seventh and the eight proposal.
* Medicine: The five best ranked proposals are funded, the five worst ranked proposals are rejected. Two proposals are randomly selected for funding between the proposals ranked as sixth to ninth.
* Biology: The eight best ranked proposals are funded, the ten worst ranked proposals are rejected, no random selection element.
* STEM: The four best ranked proposals are funded, the ten worst ranked proposals are rejected. Two proposals are randomly selected for funding between the proposals ranked as fifth to eighth. 

```{r rank-credible-intervals-pm2020, fig.height=7, fig.width=9.5, warning=FALSE, message=FALSE, fig.cap = "<span style='color: #d3d3d3;'>The expected rank together with 50% credible intervals of the rank. </span>\\label{fig:rank_credible_intervals_pm2020}",  fig.align="center"}

plot_er_dist_humanities <- 
  plot_er_distributions(get_mcmc_samples_result = mcmc_hss_h,
                        n_proposals = hss_h %>% 
                          summarise(n_distinct(proposal)) %>% pull(),
                        name_er = "rank_theta", title = "Humanities",
                        number_fundable = how_many_fundable["hss_h"],
                        outer_show = FALSE, proposal = "")
 

plot_er_dist_social_sciences <- 
  plot_er_distributions(get_mcmc_samples_result = mcmc_hss_s,
                        n_proposals = hss_s %>% 
                          summarise(n_distinct(proposal)) %>% pull(),
                        name_er = "rank_theta", title = "Social Sciences",
                        number_fundable = how_many_fundable["hss_s"],
                        outer_show = FALSE, proposal = "")
plot_er_dist_biology <- 
  plot_er_distributions(get_mcmc_samples_result = mcmc_ls_b,
                        n_proposals = ls_b %>% 
                          summarise(n_distinct(proposal)) %>% pull(),
                        name_er = "rank_theta", title = "Biology",
                        number_fundable = how_many_fundable["ls_b"],
                        outer_show = FALSE, proposal = "")

plot_er_dist_medicine <- 
  plot_er_distributions(get_mcmc_samples_result = mcmc_ls_m,
                        n_proposals = ls_m %>% 
                          summarise(n_distinct(proposal)) %>% pull(),
                        name_er = "rank_theta", title = "Medicine",
                        number_fundable = how_many_fundable["ls_m"],
                        outer_show = FALSE, proposal = "")

plot_er_dist_stem <- 
  plot_er_distributions(get_mcmc_samples_result = mcmc_stem,
                        n_proposals = stem %>% 
                          summarise(n_distinct(proposal)) %>% pull(),
                        name_er = "rank_theta", title = "STEM",
                        number_fundable = how_many_fundable["stem"],
                        outer_show = FALSE, proposal = "")

leg <- cowplot::get_legend(plot_er_dist_stem)
gridExtra::grid.arrange(plot_er_dist_humanities + 
                          theme(legend.position = "none",
                                title = element_text(size = 7)),
                        plot_er_dist_social_sciences + 
                          theme(legend.position = "none",
                                title = element_text(size = 7)),
                        plot_er_dist_medicine + 
                          theme(legend.position = "none",
                                title = element_text(size = 7)),
                        plot_er_dist_biology + 
                          theme(legend.position = "none",
                                title = element_text(size = 7)),
                        plot_er_dist_stem + 
                          theme(legend.position = "none",
                                title = element_text(size = 7)), leg, ncol = 3)
```


Finally, Figure \@ref(fig:voter-behaviour-plot-pm2020) shows the posterior distributions of the $\nu_j$'s for all PM panels, estimated from the discussed proposals. If for a specific voter $\nu_j$ is on average negative, this means that voter $j$ is stricter compared to the other voters in the panel and his/her stricter grades are corrected more. On the other hand, a $\nu_j$ that is on average positive reflects the behaviour of a voter who gives generally higher grades compared to the remaining voters. In the STEM panel, we can observe an extreme voter (voter 18). Looking at the raw data, this makes total sense, as voter 18 gives especially low grades compared to the other voters (Cs to proposals to which others gave As and ABs).

```{r voter-behaviour-plot-pm2020, fig.height=7, fig.width=9.5, warning=FALSE, message=FALSE, fig.cap = "<span style='color: #d3d3d3;'>Posterior distributions of the referee-specific means $\\nu_j$ in the Social Sciences panel. Each of the 14 referees voted on up to 18 proposals, unless they had a conflict of interest or other reasons to be absent during the vote.</span> \\label{fig:voter_behav_ER_pm_2020}",  fig.align="center"}

plot_voter_humanities <- 
  voter_behavior_distribution(get_mcmc_samples_result = mcmc_hss_h,
                              title = "Humanities",
                              n_voters = hss_h %>% 
                                summarise(n_distinct(voter)) %>% pull(),
                              xlim_min = -1, xlim_max = 1,
                              scale = 1)
plot_voter_social_sciences <- 
  voter_behavior_distribution(get_mcmc_samples_result = mcmc_hss_s,
                              title = "Social Sciences",
                              n_voters = hss_s %>% 
                                summarise(n_distinct(voter)) %>% pull(),
                              xlim_min = -1, xlim_max = 1,
                              scale = 1)
plot_voter_biology <- 
  voter_behavior_distribution(get_mcmc_samples_result = mcmc_ls_b,
                              title = "Biology",
                              n_voters = ls_b %>% 
                                summarise(n_distinct(voter)) %>% pull(),
                              xlim_min = -1, xlim_max = 1,
                              scale = 1)
plot_voter_medicine <- 
  voter_behavior_distribution(get_mcmc_samples_result = mcmc_ls_m,
                              title = "Medicine",
                              n_voters = ls_m %>% 
                                summarise(n_distinct(voter)) %>% pull(),
                              xlim_min = -1, xlim_max = 1,
                              scale = 1)
plot_voter_stem <- 
  voter_behavior_distribution(get_mcmc_samples_result = mcmc_stem,
                              title = "STEM",
                              n_voters = stem %>% 
                                summarise(n_distinct(voter)) %>% pull(),
                              xlim_min = -1.2, xlim_max = 1,
                              scale = 1)

gridExtra::grid.arrange(plot_voter_humanities, plot_voter_social_sciences,
                        plot_voter_medicine, plot_voter_biology,
                        plot_voter_stem, ncol = 5)

```

Instead of defining the random selection group using the ERs, we can directly use the proposal effects ($\theta_i$'s). Figure \@ref(fig:rank-credible-intervals-theta-pm2020) shows the $\theta_i$'s together with their 50%-Crl, the higher the $\theta_i$ the better the quality of the proposal. The division into the maximum of three groups (accepted, rejected and random selection) is the same using the proposal effects as with the expected rank. 

```{r rank-credible-intervals-theta-pm2020, fig.height=7, fig.width=9.5, echo = FALSE, warning=FALSE, message=FALSE, fig.cap = "<span style='color: #d3d3d3;'>The means of the posterior distributions of the $\\theta_i$'s together with their 50% credible intervals for the PM fellowship proposals. The dashed blue line is the naive funding line, $e.g$ the posterior mean of the distribution of the $\\theta_i$ of the last fundable proposal.</span> \\label{fig:rank_credible_intervals_theta_pm2020}"}

plot_theta_dist_humanities <- 
  plot_er_distributions(get_mcmc_samples_result = mcmc_hss_h,
                        n_proposals = hss_h %>% 
                          summarise(n_distinct(proposal)) %>% pull(),
                        name_er_or_theta = "application_intercept", 
                        title = "Humanities", er = FALSE,
                        number_fundable = how_many_fundable["hss_h"],
                        outer_show = FALSE, proposal = "",
                        ylab = expression(theta[i]))

plot_theta_dist_social_sciences <- 
  plot_er_distributions(get_mcmc_samples_result = mcmc_hss_s,
                        n_proposals = hss_s %>% 
                          summarise(n_distinct(proposal)) %>% pull(),
                        name_er_or_theta = "application_intercept", 
                        er = FALSE, title = "Social Sciences",
                        number_fundable = how_many_fundable["hss_s"],
                        outer_show = FALSE, proposal = "",
                        ylab = expression(theta[i])) 
plot_theta_dist_biology <- 
  plot_er_distributions(get_mcmc_samples_result = mcmc_ls_b,
                        n_proposals = ls_b %>% 
                          summarise(n_distinct(proposal)) %>% pull(),
                        name_er_or_theta = "application_intercept",
                        er = FALSE, title = "Biology",
                        number_fundable = how_many_fundable["ls_b"],
                        outer_show = FALSE, proposal = "", 
                        ylab = expression(theta[i])) 

plot_theta_dist_medicine <- 
  plot_er_distributions(get_mcmc_samples_result = mcmc_ls_m,
                        n_proposals = ls_m %>% 
                          summarise(n_distinct(proposal)) %>% pull(),
                        name_er_or_theta = "application_intercept",
                        er = FALSE, title = "Medicine",
                        number_fundable = how_many_fundable["ls_m"],
                        outer_show = FALSE, proposal = "",
                        ylab = expression(theta[i]))

plot_theta_dist_stem <- 
  plot_er_distributions(get_mcmc_samples_result = mcmc_stem,
                        n_proposals = stem %>% 
                          summarise(n_distinct(proposal)) %>% pull(),
                        name_er_or_theta = "application_intercept",
                        er = FALSE, title = "STEM",
                        number_fundable = how_many_fundable["stem"],
                        outer_show = FALSE, proposal = "",
                        ylab = expression(theta[i])) 

gridExtra::grid.arrange(plot_theta_dist_humanities + 
                          theme(legend.position = "none"),
                        plot_theta_dist_social_sciences + 
                          theme(legend.position = "none"),
                        plot_theta_dist_medicine + 
                          theme(legend.position = "none"),
                        plot_theta_dist_biology + 
                          theme(legend.position = "none"),
                        plot_theta_dist_stem + 
                          theme(legend.position = "none"), leg, ncol = 2)
```

Figure \@ref(fig:rank-credible-intervals-pm2020_ordinal) shows the ER proposals together with their 50\% credible intervals when using an Bayesian hierarchical model for ordinal outcome variable.

```{r rank-credible-intervals-pm2020-ordinal, fig.height=7, fig.width=9.5, warning=FALSE, message=FALSE,  fig.cap = "<span style='color: #d3d3d3;'>The expected rank together with 50% credible intervals of the rank using an ordinal outcome model. </span>\\label{fig:rank_credible_intervals_pm2020_ordinal}", cache = TRUE}
# Ordinal version of the JAGS model:
cat("model{
      # Likelihood:
      for (i in 1:n) { # i is not the application but the review
          grade[i] ~ dinterval(latent_trait[i], c[])
          latent_trait[i] ~ dnorm(mu[i], inv_sigma2)
          mu[i] <- overall_mean + application_intercept[num_application[i]] +
            voter_intercept[num_application[i], num_voter[i]]
      }
      # Ranks:
      rank_theta[1:n_application] <- rank(-application_intercept[])
      # Priors:
      for (j in 1:n_application){
        application_intercept[j] ~ dnorm(0, inv_tau_application2)
      }
      for (l in 1:n_voters){
        for(j in 1:n_application){
          voter_intercept[j, l] ~ dnorm(nu[l], inv_tau_voter2)
        }
      }
      for (l in 1:n_voters){
        nu[l] ~ dnorm(0, 4) # 1/(0.5^2) = 4
      }
      
      for (k in 1:5){
        cc[k] ~ dunif(-1000, 1000)
      }
      c[1:5] <- sort(cc) 
      
      sigma ~ dunif(0.000001, 2)
      inv_sigma2 <- pow(sigma, -2)
      inv_tau_application2 <- pow(tau_application, -2)
      tau_application ~ dunif(0.000001, 2)
      inv_tau_voter2 <- pow(tau_voter, -2)
      tau_voter ~ dunif(0.000001, 2)
    }",
    file = here("jags_model_ordinal_interval.txt"))
seed_ordinal <- seed + 1
mcmc_hss_s_ordinal <- 
  get_mcmc_samples(data = hss_s,
                   id_application = "proposal",
                   id_voter = "voter", 
                   grade_variable = "num_grade",
                   n_chains = n.chains, n_iter = n.iter,
                   n_burnin = n.burnin,
                   other_variables = "nu",
                   ordinal_scale = TRUE, point_scale = 6,
                   path_to_jags_model = 
                     here("jags_model_ordinal_interval.txt"),
                   seed = seed_ordinal, quiet = TRUE)
mcmc_hss_h_ordinal <- 
  get_mcmc_samples(data = hss_h,
                   id_application = "proposal",
                   id_voter = "voter", 
                   grade_variable = "num_grade",
                   n_chains = n.chains, n_iter = n.iter,
                   n_burnin = n.burnin,
                   other_variables = "nu",
                   ordinal_scale = TRUE, point_scale = 6,
                   path_to_jags_model = 
                     here("jags_model_ordinal_interval.txt"),
                   seed = seed_ordinal, quiet = TRUE)

mcmc_ls_b_ordinal <- 
  get_mcmc_samples(data = ls_b,
                   id_application = "proposal",
                   id_voter = "voter", 
                   grade_variable = "num_grade",
                   n_chains = n.chains, n_iter = n.iter,
                   n_burnin = n.burnin,
                   other_variables = "nu",
                   ordinal_scale = TRUE, point_scale = 6,
                   path_to_jags_model = 
                     here("jags_model_ordinal_interval.txt"),
                   seed = seed_ordinal, quiet = TRUE)

mcmc_ls_m_ordinal <- 
  get_mcmc_samples(data = ls_m,
                   id_application = "proposal",
                   id_voter = "voter", 
                   grade_variable = "num_grade",
                   n_chains = n.chains, n_iter = n.iter,
                   n_burnin = n.burnin,
                   other_variables = "nu",
                   ordinal_scale = TRUE, point_scale = 6,
                   path_to_jags_model = 
                     here("jags_model_ordinal_interval.txt"),
                   seed = seed_ordinal, quiet = TRUE)
mcmc_stem_ordinal <- 
  get_mcmc_samples(data = stem,
                   id_application = "proposal",
                   id_voter = "voter", 
                   grade_variable = "num_grade",
                   n_chains = n.chains, n_iter = n.iter,
                   n_burnin = n.burnin,
                   other_variables = "nu",
                   ordinal_scale = TRUE, point_scale = 6,
                   path_to_jags_model = 
                     here("jags_model_ordinal_interval.txt"),
                   seed = seed_ordinal, quiet = TRUE)

plot_er_dist_humanities_ordinal <- 
  plot_er_distributions(get_mcmc_samples_result = mcmc_hss_h_ordinal,
                        n_proposals = hss_h %>% 
                          summarise(n_distinct(proposal)) %>% pull(),
                        name_er_or_theta = "rank_theta", title = "Humanities",
                        number_fundable = how_many_fundable["hss_h"],
                        outer_show = FALSE, proposal = "") 

plot_er_dist_social_sciences_ordinal <- 
  plot_er_distributions(get_mcmc_samples_result = mcmc_hss_s_ordinal,
                        n_proposals = hss_s %>% 
                          summarise(n_distinct(proposal)) %>% pull(),
                        name_er_or_theta = "rank_theta", title = "Social Sciences",
                        number_fundable = how_many_fundable["hss_s"],
                        outer_show = FALSE, proposal = "")

plot_er_dist_biology_ordinal <- 
  plot_er_distributions(get_mcmc_samples_result = mcmc_ls_b_ordinal,
                        n_proposals = ls_b %>% 
                          summarise(n_distinct(proposal)) %>% pull(),
                        name_er_or_theta = "rank_theta", title = "Biology",
                        number_fundable = how_many_fundable["ls_b"],
                        outer_show = FALSE, proposal = "")

plot_er_dist_medicine_ordinal <- 
  plot_er_distributions(get_mcmc_samples_result = mcmc_ls_m_ordinal,
                        n_proposals = ls_m %>% 
                          summarise(n_distinct(proposal)) %>% pull(),
                        name_er_or_theta = "rank_theta", title = "Medicine",
                        number_fundable = how_many_fundable["ls_m"],
                        outer_show = FALSE, proposal = "")

plot_er_dist_stem_ordinal <- 
  plot_er_distributions(get_mcmc_samples_result = mcmc_stem_ordinal,
                        n_proposals = stem %>% 
                          summarise(n_distinct(proposal)) %>% pull(),
                        name_er_or_theta = "rank_theta", title = "STEM",
                        number_fundable = how_many_fundable["stem"],
                        outer_show = FALSE, proposal = "")

leg <- cowplot::get_legend(plot_er_dist_stem_ordinal)
gridExtra::grid.arrange(plot_er_dist_humanities_ordinal + 
                          theme(legend.position = "none"),
                        plot_er_dist_social_sciences_ordinal + 
                          theme(legend.position = "none"),
                        plot_er_dist_medicine_ordinal + 
                          theme(legend.position = "none"),
                        plot_er_dist_biology_ordinal + 
                          theme(legend.position = "none"),
                        plot_er_dist_stem_ordinal + 
                          theme(legend.position = "none"), leg, ncol = 2)
```

As discussed in the main paper, other quantities derived from the ER can be of interest. The ordered PCER of the proposals in the different panels can be found in Figure \@ref{fig:pcer_pm_2020}. Note that the lower the PCER, the better the quality of the proposal compared to all competing proposals. The lowest PCER in the Medicine panel is for example `r ranks_ls_m$rankings %>% pull(pcer) %>% min() %>% round(1)`%, meaning that the probability that the best ranked proposal is of worse quality compared to a randomly selected proposal (including itself) is only `r ranks_ls_m$rankings %>% pull(pcer) %>% min() %>% round(1)`%.

```{r pcer-plot-pm2020, fig.height=7, echo = FALSE, fig.width=9.5, warning=FALSE, message=FALSE, fig.cap = "<span style='color: #d3d3d3;'>The PCER of the ranked proposals for all the PM panels. </span> \\label{fig:pcer_pm_2020}", fig.align="center"}

plot_pcer_humanities <- ranks_hss_h$rankings %>% 
  arrange(pcer) %>% 
  ggplot(aes(x = 1:11, y = pcer/100)) +
  geom_point() + 
  theme_minimal() + 
  labs(x = " ",
       y = "PCER",
       title = "Humanities") +
  theme(legend.position = "none",
        axis.ticks.x = element_line(color = "darkgray"),
        panel.grid.minor.x = element_blank(),
        panel.grid.minor.y = element_blank()) +
  scale_y_continuous(limits = c(0, 1), labels = scales::percent) +
  scale_x_continuous(breaks = 1:11,
                     labels = 1:11)

plot_pcer_soc_sciences <- ranks_hss_s$rankings %>% 
  arrange(pcer) %>% 
  ggplot(aes(x = 1:18, y = pcer/100)) +
  geom_point() + 
  theme_minimal() + 
  labs(x = " ",
       y = " ",
       title = "Social Sciences") +
  theme(legend.position = "none",
        axis.ticks.x = element_line(color = "darkgray"),
        panel.grid.minor.x = element_blank(),
        panel.grid.minor.y = element_blank()) +
  scale_y_continuous(limits = c(0, 1), labels = scales::percent) +
  scale_x_continuous(breaks = 1:18,
                     labels = 1:18)

plot_pcer_biology <- ranks_ls_b$rankings %>% 
  arrange(pcer) %>% 
  ggplot(aes(x = 1:18, y = pcer/100)) +
  geom_point() + 
  theme_minimal() + 
  labs(x = "Proposals ranked based on PCER",
       y = "PCER",
       title = "Biology") +
  theme(legend.position = "none",
        axis.ticks.x = element_line(color = "darkgray"),
        panel.grid.minor.x = element_blank(),
        panel.grid.minor.y = element_blank()) +
  scale_y_continuous(limits = c(0, 1), labels = scales::percent) +
  scale_x_continuous(breaks = 1:18,
                     labels = 1:18)

plot_pcer_medicine <- ranks_ls_m$rankings %>% 
  arrange(pcer) %>% 
  ggplot(aes(x = 1:14, y = pcer/100)) +
  geom_point() + 
  theme_minimal() + 
  labs(x = " ",
       y = " ",
       title = "Medicine") +
  theme(legend.position = "none",
        axis.ticks.x = element_line(color = "darkgray"),
        panel.grid.minor.x = element_blank(),
        panel.grid.minor.y = element_blank()) +
  scale_y_continuous(limits = c(0, 1), labels = scales::percent) +
  scale_x_continuous(breaks = 1:14,
                     labels = 1:14)

plot_pcer_stem <- ranks_stem$rankings %>% 
  arrange(pcer) %>% 
  ggplot(aes(x = 1:18, y = pcer/100)) +
  geom_point() + 
  theme_minimal() + 
  labs(x = "Proposals ranked based on PCER",
       y = " ",
       title = "Biology") +
  theme(legend.position = "none",
        axis.ticks.x = element_line(color = "darkgray"),
        panel.grid.minor.x = element_blank(),
        panel.grid.minor.y = element_blank()) +
  scale_y_continuous(limits = c(0, 1), labels = scales::percent) +
  scale_x_continuous(breaks = 1:18,
                     labels = 1:18)



gridExtra::grid.arrange(plot_pcer_humanities, plot_pcer_soc_sciences,
                        plot_pcer_medicine, plot_pcer_biology,
                        plot_pcer_stem, ncol = 3)

```

Another related quantity discussed in the main paper is the SUCRA, which is the surface under the cumulative ranking probability (CRP) plot. The latter can simply be plotted using the MCMC samples of the model parameters. Figures \@ref(fig:ranko-humanities) gives an example of the this plot for the Humanities panel. The SUCRA of each proposal is written insight the CRP plots. Every proposal has its own CRP plot. Like the PCER, the SUCRA is independent of the number of proposals ranked.

```{r ranko-humanities, fig.height=7, fig.width=9.5, warning=FALSE, message=FALSE, fig.cap = "<span style='color: #d3d3d3;'>Cumulative ranking probability plots for the competing proposals in the Humanities Panel.</span>\\label{fig:ranko_humanities}", fig.align="center"}
plot_rankogram(data = hss_h,
               id_application = "proposal",
               id_voter = "voter", 
               grade_variable = "num_grade",
               rank_theta_name = "rank_theta",
               cumulative_rank_prob = TRUE,
               mcmc_samples = mcmc_hss_h)
```


# Project Funding

Project funding (PF) is the SNSF's most important funding instrument. PF grants are meant for research proposals with a topic of the applicantâ€™s own choice. The data that we will analyze in the following originates from the evaluation of the April 2020 Call to the Mathematics, Natural and Engineering Sciences (MINT) Division. All in all, `r length(unique(mint_sections$proposal))` grant proposals were evaluated in the MINT division. To ensure a manageable amount of work to be spread among the panel members and the quality of the reviews and gradings, the evaluation was done in four separate sections. The sections were defined as such that they were of similar sizes and had a similar number of international members and female members. This approach ensures that the section compositions are comparable. Each section did indeed define their own funding line, while keeping the success rate in each section the same ($\sim 30\%$). Table \@ref(tab:summarising) shows the number of proposals evaluated and fundable by section (nine members are present in each section). The number of fundable proposals will be defined as $\sim 30\%$ of the number of evaluated proposals. Each section member or voter has to individually evaluate each proposal in their section (unless they have a conflict of interest or another reason to be absent during the vote). The average evaluation scores (6=A, \dots, 1=D) given to all the proposals in each section and the averages of the fundable proposals ($30\%$ best ranked according to their average score) are also shown.

```{r summarising, echo = FALSE}
mint_sections %>% 
  group_by(proposal) %>% 
  mutate(SCORE = mean(num_grade, na.rm = TRUE)) %>% 
  ungroup() %>% 
  select(proposal, section, SCORE) %>%
  unique() %>% 
  mutate(Section = factor(section, levels = c("one", "two", "three",
                                              "four"))) %>% 
  group_by(Section) %>%
  arrange(-SCORE) %>% 
  summarise("N. of proposals" = n_distinct(proposal),
            "N. of fundable proposa" = round(.3*n_distinct(proposal)),
            "Avg score" = round(mean(SCORE), 2),
            "Avg score of 30% best" = 
              round(mean(head(SCORE, round(.3*n_distinct(proposal)))), 2)) %>% 
  kableExtra::kable(caption = "Summarising statistics in the four sections of the 
               MINT division in Project Funding.\\label{tab:summarising}",
                    booktabs = TRUE) %>% 
  kable_material(c("striped", "hover"))
```

To understand how reliably the assessors' evaluations were in the different sections, we refer to Figure \@ref(fig:all-grades-mint). Compared to the previous Postdoc.Mobility case study, the intra-class correlation coefficients are higher and can be classified as good. 
One reason for the higher reliability here is that all project funding proposals are included here - also the ones with low/high average scores, for which the votes are usually less variable. (In contrast, we only looked at PM proposals with average scores in the middle range.)

```{r grade-distribution-plots-mint, fig.cap = "<span style='color: #d3d3d3;'>The individual votes given to all proposals by all panel members. The red dots represent the averages of those individual votes for each proposal. The intra-class correlation coefficients (ICC) together with their 95\\% confidence intervals of the different panels can be found in the titles of the subfigures. The overall reliability in the those panels can be classified as good.</span>\\label{fig:all-grades-mint}",fig.height=7, fig.width=9.5, warning = FALSE, fig.align = "center"}

leg <- cowplot::get_legend(get_grades_plot(mint_section1))
gridExtra::grid.arrange(get_grades_plot(mint_section1, mint_section1_mat,
                                        x_min = 1.8, title = "Section 1", 
                                        jitter_alpha = .4, jitter_w = .05) +
                          theme(legend.position = "none"),
                        get_grades_plot(mint_section2, mint_section2_mat,
                                        x_min = 1.8, title = "Section 2", 
                                        jitter_alpha = .4, jitter_w = .05) + 
                          theme(legend.position = "none"),
                        get_grades_plot(mint_section3, mint_section3_mat,
                                        x_min = 1.8, title = "Section 3", 
                                        jitter_alpha = .4, jitter_w = .05)+ 
                          theme(legend.position = "none"),
                        get_grades_plot(mint_section4, mint_section4_mat, 
                                        x_min = 1.8, title = "Section 4", 
                                        jitter_alpha = .4, jitter_w = .05) + 
                          theme(legend.position = "none")#, leg
                        )
```


Then, as for the postdoc.mobility fellowships Bayesian hierarchical models will be fitted for each of the four sections individually, but also for the pooled data and adding an additional section effect in the model definition. Afterwards the computation of the expected ranks (and the PCER) is straightforward using the MCMC samples from the latter models.


```{r compute-er-mint-data, cache = TRUE}
#### JAGS model for continuous outcome ####
cat("model{
      # Likelihood:
      for (i in 1:n) { # i is not the application but the review
          grade[i] ~ dnorm(mu[i], inv_sigma2)
                # inv_sigma2 is precision (1 / variance)
          mu[i] <- overall_mean + application_intercept[num_application[i]] +
          voter_intercept[num_application[i], num_voter[i]] +
          section_intercept[num_section[i]]
      }
      # Ranks:
      rank_theta[1:n_application] <- rank(-application_intercept[])
      # Priors:
      for (j in 1:n_application){
        application_intercept[j] ~ dnorm(0, inv_tau_application2)
      }
      for (l in 1:n_voters){
        for(j in 1:n_application){
          voter_intercept[j, l] ~ dnorm(nu[l], inv_tau_voter2)
        }
      }
      for (l in 1:n_voters){
        nu[l] ~ dnorm(0, 4) # 1/(0.5^2) = 4
      }
      for (l in 1:n_section){
        section_intercept[l] ~ dnorm(0, inv_tau_section2)
      }
      sigma ~ dunif(0.000001, 2)
      inv_sigma2 <- pow(sigma, -2)
      inv_tau_application2 <- pow(tau_application, -2)
      tau_application ~ dunif(0.000001, 2)
      inv_tau_voter2 <- pow(tau_voter, -2)
      tau_voter ~ dunif(0.000001, 2)
      inv_tau_section2 <- pow(tau_section, -2)
      tau_section ~ dunif(0.000001, 2)
    }",
    file = here("revision", "manuscript", "jags_model_with_sections.txt"))

n.iter <- 40000
n.burnin <- 20000
n.chains <- 2
seed <- 1991
mcmc_mint <- 
  ERforResearch:::get_mcmc_samples(data = mint_sections,
                                   id_application = "proposal",
                                   id_voter = "voter",
                                   id_section =  "section",
                                   grade_variable = "num_grade",
                                   tau_section_name = "tau_section",
                                   n_chains = n.chains, n_iter = n.iter,
                                   n_burnin = n.burnin,
                                   path_to_jags_model = 
                                     here("revision", "manuscript",
                                          "jags_model_with_sections.txt"),
                                   seed = seed, quiet = TRUE)
# Section by section:
mcmc_mint_section1 <-
  ERforResearch:::get_mcmc_samples(data = mint_section1,
                                   id_application = "proposal",
                                   id_voter = "voter", 
                                   grade_variable = "num_grade",
                                   n_chains = n.chains, n_iter = n.iter,
                                   n_burnin = n.burnin,
                                   seed = seed, quiet = TRUE)

mcmc_mint_section2 <- 
  ERforResearch:::get_mcmc_samples(data = mint_section2,
                                   id_application = "proposal",
                                   id_voter = "voter", 
                                   grade_variable = "num_grade",
                                   n_chains = n.chains, n_iter = n.iter,
                                   n_burnin = n.burnin,
                                   seed = seed, quiet = TRUE)

mcmc_mint_section3 <-
  ERforResearch:::get_mcmc_samples(data = mint_section3,
                                   id_application = "proposal",
                                   id_voter = "voter", 
                                   grade_variable = "num_grade",
                                   n_chains = n.chains, n_iter = n.iter,
                                   n_burnin = n.burnin,
                                   seed = seed, quiet = TRUE)

mcmc_mint_section4 <-
  ERforResearch:::get_mcmc_samples(data = mint_section4,
                                   id_application = "proposal",
                                   id_voter = "voter", 
                                   grade_variable = "num_grade",
                                   n_chains = n.chains, n_iter = n.iter,
                                   n_burnin = n.burnin,
                                   seed = seed, quiet = TRUE)


ranks_mint <- 
  get_er_from_jags(data = mint_sections,
                   id_application = "proposal",
                   id_voter = "voter",
                   id_section =  "section",
                   grade_variable = "num_grade",
                   tau_section_name = "tau_section",
                   mcmc_samples = mcmc_mint,
                   seed = seed)

# Section by section: 

ranks_mint_section1 <-
  get_er_from_jags(data = mint_section1,
                   id_application = "proposal",
                   id_voter = "voter", 
                   grade_variable = "num_grade",
                   mcmc_samples = mcmc_mint_section1,
                   seed = seed)

ranks_mint_section2 <- 
  get_er_from_jags(data = mint_section2,
                   id_application = "proposal",
                   id_voter = "voter", 
                   grade_variable = "num_grade",
                   mcmc_samples = mcmc_mint_section2,
                   seed = seed)

ranks_mint_section3 <-
  get_er_from_jags(data = mint_section3,
                   id_application = "proposal",
                   id_voter = "voter", 
                   grade_variable = "num_grade",
                   mcmc_samples = mcmc_mint_section3,
                   seed = seed)

ranks_mint_section4 <-
  get_er_from_jags(data = mint_section4,
                   id_application = "proposal",
                   id_voter = "voter", 
                   grade_variable = "num_grade",
                   mcmc_samples = mcmc_mint_section4,
                   seed = seed)

#### JAGS model for ordinal outcome ####
cat("model{
      # Likelihood:
      for (i in 1:n) { # i is not the application but the review
          grade[i] ~ dinterval(latent_trait[i], c[])
          latent_trait[i] ~ dnorm(mu[i], inv_sigma2)
          mu[i] <- overall_mean + application_intercept[num_application[i]] +
            voter_intercept[num_application[i], num_voter[i]] +
          section_intercept[num_section[i]]
      }
      # Ranks:
      rank_theta[1:n_application] <- rank(-application_intercept[])
      # Priors:
      for (j in 1:n_application){
        application_intercept[j] ~ dnorm(0, inv_tau_application2)
      }
      for (l in 1:n_voters){
        for(j in 1:n_application){
          voter_intercept[j, l] ~ dnorm(nu[l], inv_tau_voter2)
        }
      }
      for (l in 1:n_voters){
        nu[l] ~ dnorm(0, 4) # 1/(0.5^2) = 4
      }
      
      for (l in 1:n_section){
        section_intercept[l] ~ dnorm(0, inv_tau_section2)
      }
      
      for (k in 1:5){
        cc[k] ~ dunif(-1000, 1000)
      }
      c[1:5] <- sort(cc) 
      
      sigma ~ dunif(0.000001, 2)
      inv_sigma2 <- pow(sigma, -2)
      inv_tau_application2 <- pow(tau_application, -2)
      tau_application ~ dunif(0.000001, 2)
      inv_tau_voter2 <- pow(tau_voter, -2)
      tau_voter ~ dunif(0.000001, 2)
      inv_tau_section2 <- pow(tau_section, -2)
      tau_section ~ dunif(0.000001, 2)
    }",
    file = here("revision", "manuscript",
                "jags_model_with_sections_ordinal.txt"))

seed_ordinal <- seed + 1
mcmc_mint_ordinal <-
  get_mcmc_samples(data = mint_sections,
                   id_application = "proposal",
                   id_voter = "voter",
                   id_section =  "section",
                   grade_variable = "num_grade",
                   tau_section_name = "tau_section",
                   n_chains = n.chains, n_iter = n.iter,
                   n_burnin = n.burnin,
                   ordinal_scale = TRUE,
                   point_scale = 6,
                   path_to_jags_model =
                     here("revision", "manuscript",
                          "jags_model_with_sections_ordinal.txt"),
                   seed = seed_ordinal, quiet = TRUE)
# Section by Section: 
mcmc_mint_section1_ordinal <-
  get_mcmc_samples(data = mint_section1,
                   id_application = "proposal",
                   id_voter = "voter",
                   grade_variable = "num_grade",
                   n_chains = n.chains, n_iter = n.iter,
                   n_burnin = n.burnin,
                   ordinal_scale = TRUE,
                   point_scale = 6,
                   path_to_jags_model =
                     here("revision", "manuscript",
                          "jags_model_ordinal_interval.txt"),
                   seed = seed_ordinal, quiet = TRUE)

mcmc_mint_section2_ordinal <-
    get_mcmc_samples(data = mint_section2,
                     id_application = "proposal",
                     id_voter = "voter",
                     grade_variable = "num_grade",
                     n_chains = n.chains, n_iter = n.iter,
                     n_burnin = n.burnin,
                     ordinal_scale = TRUE,
                     point_scale = 6,
                     path_to_jags_model =
                       here("revision", "manuscript",
                            "jags_model_ordinal_interval.txt"),
                     seed = seed_ordinal, quiet = TRUE)

mcmc_mint_section3_ordinal <-
  get_mcmc_samples(data = mint_section3,
                   id_application = "proposal",
                   id_voter = "voter",
                   grade_variable = "num_grade",
                   n_chains = n.chains, n_iter = n.iter,
                   n_burnin = n.burnin,
                   ordinal_scale = TRUE,
                   point_scale = 6,
                   path_to_jags_model =
                     here("revision", "manuscript",
                          "jags_model_ordinal_interval.txt"),
                   seed = seed_ordinal, quiet = TRUE)

mcmc_mint_section4_ordinal <-
    get_mcmc_samples(data = mint_section4,
                     id_application = "proposal",
                     id_voter = "voter",
                     grade_variable = "num_grade",
                     n_chains = n.chains, n_iter = n.iter,
                     n_burnin = n.burnin,
                     ordinal_scale = TRUE,
                     point_scale = 6,
                     path_to_jags_model =
                       here("revision", "manuscript",
                            "jags_model_ordinal_interval.txt"),
                     seed = seed_ordinal, quiet = TRUE)

ranks_mint_ordinal <-
  get_er_from_jags(data = mint_sections,
                   id_application = "proposal",
                   id_voter = "voter",
                   id_section =  "section",
                   grade_variable = "num_grade",
                   tau_section_name = "tau_section",
                   mcmc_samples = mcmc_mint_ordinal,
                   ordinal_scale = TRUE,
                   point_scale = 6,
                   seed = seed)

# Section by Section:
ranks_mint_section1_ordinal <-
  get_er_from_jags(data = mint_section1,
                   id_application = "proposal",
                   id_voter = "voter",
                   grade_variable = "num_grade",
                   mcmc_samples = mcmc_mint_section1_ordinal,
                   ordinal_scale = TRUE,
                   point_scale = 6,
                   seed = seed_ordinal)

ranks_mint_section2_ordinal <-
  get_er_from_jags(data = mint_section2,
                   id_application = "proposal",
                   id_voter = "voter",
                   grade_variable = "num_grade",
                   mcmc_samples = mcmc_mint_section2_ordinal,
                   seed = seed_ordinal)

ranks_mint_section3_ordinal <-
  get_er_from_jags(data = mint_section3,
                   id_application = "proposal",
                   id_voter = "voter",
                   grade_variable = "num_grade",
                   mcmc_samples = mcmc_mint_section3_ordinal,
                   seed = seed_ordinal)

ranks_mint_section4_ordinal <-
  get_er_from_jags(data = mint_section4,
                   id_application = "proposal",
                   id_voter = "voter",
                   grade_variable = "num_grade",
                   mcmc_samples = mcmc_mint_section4_ordinal,
                   seed = seed_ordinal)
```

Figure \@ref(fig:mint-er-plot) shows the resulting expected ranks for each of the sections separately. The proposals are first ranked based on their average score (Fixed Ranking) and the posterior mean of the score. The latter is adjusted for random variation (with a random intercept for the proposal), and the referee behavior (through a random intercept for the voter). Remember that, each section will define a separate funding line. Note that rank 1 is reserved for the best proposal according to its quality assessment. On top of these figures, we can read of the rankability which is very high. This means, that given the many proposals that have to be ranked in each section, the Bayesian hierarchical models are able to explain most of the variation; the estimated proposal effects are close to the true effect and less clouded by random variation. This is also shown in the Figure by the fact that the ER do not quickly converge to the median. The best ERs are `r ranks_mint_section1$rankings$er %>% min() %>% round(2)` for section one, `r ranks_mint_section2$rankings$er %>% min() %>% round(2)` for section two, `r ranks_mint_section3$rankings$er %>% min() %>% round(2)` for section three and `r ranks_mint_section4$rankings$er %>% min() %>% round(2)` for section four; so very close to 1. The worst ERs are `r ranks_mint_section1$rankings$er %>% max() %>% round(2)` for section one, `r ranks_mint_section2$rankings$er %>% max() %>% round(2)` for section two, `r ranks_mint_section3$rankings$er %>% max() %>% round(2)` for section three and `r ranks_mint_section4$rankings$er %>% max() %>% round(2)` for section four, which are all very close to the total number of submissions. 


```{r mint-er-plot, fig.cap = "<span style='color: #d3d3d3;'>The proposals to the different sections in the MINT division ranked based on the average overall score (Fixed Ranking), the posterior means and the expected rank (ER). The color code tells us which proposals could be funded (orange) to ensure a 30\\% success rate.</span>\\label{fig:rank_mint_section}", fig.height=7, fig.width=9.5, warning = FALSE, fig.align = "center"}
x1 <- round(.3 * (mint_section1 %>% 
  pull(proposal) %>% 
  unique() %>% 
  length()))

p1 <- plotting_er_results(ranks_mint_section1, title = "Section 1",
                          result_show = FALSE, how_many_fundable = x1,
                          draw_funding_line = FALSE) 

x2 <- round(.3 * (mint_section2 %>% 
  pull(proposal) %>% 
  unique() %>% 
  length()))

p2 <- plotting_er_results(ranks_mint_section2, title = "Section 2",
                          result_show = FALSE, how_many_fundable = x2,
                          draw_funding_line = FALSE) 

x3 <- round(.3 * (mint_section3 %>% 
  pull(proposal) %>% 
  unique() %>% 
  length()))

p3 <- plotting_er_results(ranks_mint_section3, title = "Section 3",
                          result_show = FALSE, how_many_fundable = x3,
                          draw_funding_line = FALSE) 

x4 <- round(.3 * (mint_section4 %>% 
  pull(proposal) %>% 
  unique() %>% 
  length()))

p4 <- plotting_er_results(ranks_mint_section4, title = "Section 4",
                          result_show = FALSE, how_many_fundable = x4,
                          draw_funding_line = FALSE) 

leg <- cowplot::get_legend(p1)
gridExtra::grid.arrange(p1 + 
                          theme(legend.position = "none",
                                axis.text.y = element_blank(),
                                axis.title.x = element_blank()),
                        p2 + 
                          theme(legend.position = "none",
                                axis.text.y = element_blank(),
                                axis.title.x = element_blank()),
                        p3 + 
                          theme(legend.position = "none",
                                axis.text.y = element_blank(),
                                axis.title.x = element_blank()),
                        p4 + 
                          theme(legend.position = "none",
                                axis.text.y = element_blank(),
                                axis.title.x = element_blank()))

```


To better decide on which proposals should be funded in each section individually, and where, if needed, a random selection group should be defined, consider Figure \ref{fig:ER_dist_all_mint_ind}. Hence, looking at the section seperately would lead to a random selection group of 9 in Section 1, which is `r round(9/87*100, 1)`% of all proposals evaluated in this section; 5 (`r round(5/9*100, 1)`%) of these can still be funded. No random selection group is needed for section 2 and 3, while three more proposals would be selected among the six proposal in the random selection group for section 4. 

```{r er-rank-distribution-mint-ind, fig.height=8, fig.width=9, echo = FALSE, warning=FALSE, message=FALSE, fig.cap = "<span style='color: #d3d3d3;'>The ER with 50\\% credible intervals. The color code shows the final group (Accepted, Rejected or Random Selection).</span>\\label{fig:ER_dist_all_mint_ind}", fig.align="center"}

n_applications1 <- mint_section1 %>% 
  summarise(n_distinct(proposal)) %>% 
  pull()

n_applications2 <- mint_section2 %>% 
  summarise(n_distinct(proposal)) %>% 
  pull()

n_applications3 <- mint_section3 %>% 
  summarise(n_distinct(proposal)) %>% 
  pull()

n_applications4 <- mint_section4 %>% 
  summarise(n_distinct(proposal)) %>% 
  pull()
  

plot_er_dist_mint1 <-
  plot_er_distributions(mcmc_mint_section1, n_proposals = n_applications1,
                        outer_show = FALSE, 
                        number_fundable = x1, size_pt = .5, size_inner = 1) +
  labs(title = "Section 1") +
  theme(panel.grid.major.x = element_blank(),
        axis.text.x = element_blank())

zoomtheme <- theme(legend.position = "none", axis.line = element_blank(),
                   axis.text.x = element_blank(), axis.text.y = element_blank(),
                   axis.ticks = element_blank(), axis.title.x = element_blank(),
                   axis.title.y = element_blank(), title = element_blank(),
                   panel.grid.major = element_blank(),
                   panel.grid.minor = element_blank(),
                   panel.background = element_rect(color = 'red',
                                                   fill = "white"))

p.zoomrast1 <- plot_er_dist_mint1 + 
  xlim(levels(plot_er_dist_mint1$data$parameter)[20:45]) +
  ylim(c(15, 48)) + 
  zoomtheme

plot_er_dist_mint2 <-
  plot_er_distributions(mcmc_mint_section2, n_proposals = n_applications2,
                        outer_show = FALSE, 
                        number_fundable = x2, size_pt = .5, size_inner = 1) +
  labs(title = "Section 2") +
  theme(panel.grid.major.x = element_blank(),
        axis.text.x = element_blank())

p.zoomrast2 <- plot_er_dist_mint2 + 
  xlim(levels(plot_er_dist_mint2$data$parameter)[20:45]) +
  ylim(c(12, 48)) + 
  zoomtheme

plot_er_dist_mint3 <-
  plot_er_distributions(mcmc_mint_section3, n_proposals = n_applications3,
                        outer_show = FALSE, 
                        number_fundable = x3, size_pt = .5, size_inner = 1) +
  labs(title = "Section 3") +
  theme(panel.grid.major.x = element_blank(),
        axis.text.x = element_blank())

p.zoomrast3 <- plot_er_dist_mint3 + 
  xlim(levels(plot_er_dist_mint3$data$parameter)[20:40]) +
  ylim(c(15, 45)) + 
  zoomtheme

plot_er_dist_mint4 <-
  plot_er_distributions(mcmc_mint_section4, n_proposals = n_applications4,
                        outer_show = FALSE, 
                        number_fundable = x4, size_pt = .5, size_inner = 1) +
  labs(title = "Section 4") +
  theme(panel.grid.major.x = element_blank(),
        axis.text.x = element_blank())

p.zoomrast4 <- plot_er_dist_mint4 + 
  xlim(levels(plot_er_dist_mint4$data$parameter)[20:40]) +
  ylim(c(15, 45)) + 
  zoomtheme

gridExtra::grid.arrange(
  plot_er_dist_mint1 + 
    theme(legend.position = "top") +
    annotation_custom(grob = ggplotGrob(p.zoomrast1),
                      xmin = 53, xmax = 80, 
                      ymin = min(ranks_mint_section1$rankings$er),
                      ymax = min(ranks_mint_section1$rankings$er) + 50),
  
  plot_er_dist_mint2 + 
    theme(legend.position = "none") +
    annotation_custom(grob = ggplotGrob(p.zoomrast2),
                      xmin = 53, xmax = 80, 
                      ymin = min(ranks_mint_section2$rankings$er),
                      ymax = min(ranks_mint_section2$rankings$er) + 50),
  
  plot_er_dist_mint3 + 
    theme(legend.position = "none") +
    annotation_custom(grob = ggplotGrob(p.zoomrast3),
                      xmin = 53, xmax = 80, 
                      ymin = min(ranks_mint_section3$rankings$er),
                      ymax = min(ranks_mint_section3$rankings$er) + 50),
  
  plot_er_dist_mint4 + 
    theme(legend.position = "none") +
    annotation_custom(grob = ggplotGrob(p.zoomrast4),
                      xmin = 53, xmax = 80, 
                      ymin = min(ranks_mint_section4$rankings$er),
                      ymax = min(ranks_mint_section4$rankings$er) + 50), ncol = 2)
```


As the ER methodology is able to account for further structural differences, the proposals can also be ranked all together. When incorporating all the information from the four sections in the same model, we have to account for the hierarchical structure based on the sections. Certainly, there are different dynamics in the sections that might have an influence on the final score, but are independent of the actual quality of the discussed proposals. As can be seen in Table \@ref(tab:summarising) the score distributions were not exactly the same in the different sections, with section three giving higher scores on average, even though the sections were constructed as to being comparable. Therefore, a random intercept for the section can be introduced in the Bayesian hierarchichal model to adjust for those dynamics.

```{r mint-er-plot-all, echo = FALSE, fig.cap = "<span style='color: #d3d3d3;'>All the proposals to the MINT division (from all sections) ranked based on the average overall score (Fixed Ranking), the posterior means and the expected rank (ER). The color code tells us which proposals could be funded (blue) to ensure a 30\\% success rate.</span>\\label{fig:rank_mint_all}", fig.height = 8, fig.width= 6, warning = FALSE, fig.align="center"}
x <- round(.3*n_distinct(mint_sections$proposal))

plotting_er_results(ranks_mint, title = "MINT (all sections)",
                    result_show = FALSE, how_many_fundable = x,
                    draw_funding_line = FALSE) +
  theme(axis.text.y = element_blank())
```


Figure \@ref(fig:mint-er-distr-plot-all) (A) shows the resulting expected ranks. This figure mainly helps to get a first impression of whether the evaluation procedure manages to find substantial differences between proposals. We can observe the creation of several clusters. The right side of the Figure (B) shows the same ER ordered from the best ranked proposal (left) to the worst (right) together with their 50% credible intervals. The naive funding line is defined as the ER of the last fundable proposal: the `r x`th (30%  of `r n_distinct(mint_sections$proposal)`) best ranked, according to its ER. A zoom into this last figure helps to find a cluster of proposals with credible intervals overlapping the naive funding line for which a random selection should be applied to decide upon the last remaining proposals to fund. 13 of the proposals in this zoomed part have their credible interval crossing the naive funding line, and would therefore form the random selection group. Seven of these 13 proposals will be selected for funding, which is `r round(7/13*100, 1)`%. 

```{r mint-er-distr-plot-all, echo = FALSE, fig.cap = "<span style='color: #d3d3d3;'>(A) All the proposals to the MINT division (from all sections) ranked based on the average overall score (Fixed), the posterior means and the expected rank. The color code tells us which proposals could be funded (orange) to ensure a 30\\% success rate.\\label{fig:rank_mint_all} (B) Shows the same ER ordered from the best (rank 1) to the worst (rank 352) together with their 50% credible intervals. A naive funding line to ensure a 30% success rate is drawn (blue dashed line).</span>", fig.height = 6, fig.width = 7, warning = FALSE, fig.align = "center"}
plot_er_mint <- 
  plotting_er_results(ranks_mint, title = "(A)",
                      result_show = FALSE, rankability = TRUE,
                      pt_size = .3, line_size = .1, alpha_line = .7, 
                      alpha_pt = .8, how_many_fundable = x,
                      draw_funding_line = FALSE) +
  theme(axis.text.y = element_blank())


plot_er_cred_mint <- 
  plot_er_distributions(get_mcmc_samples_result = mcmc_mint,
                        n_proposals = mint_sections %>% 
                          summarise(n_distinct(proposal)) %>% pull(),
                        name_er_or_theta = "rank_theta", title = "(B)",
                        names_proposals = mint_sections %>% 
                          pull(proposal) %>% unique(),
                        number_fundable = x,
                        size_pt = .1, size_outer = 0, size_inner = .3,
                        alpha_inner = .8, outer_show = FALSE) +
  theme(axis.text.x = element_blank(),
        panel.grid.major.x = element_blank(),
        axis.title.y = element_text(size = 7, vjust = 2))

p.zoomrast <- plot_er_cred_mint + 
  xlim(levels(plot_er_cred_mint$data$parameter)[90:121]) +
  ylim(c(74, 140)) + 
  zoomtheme
gridExtra::grid.arrange(
  plot_er_mint + 
    theme(legend.position = "none"),
  plot_er_cred_mint + 
    theme(legend.position = "none") +
    annotation_custom(grob = ggplotGrob(p.zoomrast),
                      xmin = 160, xmax = 360, 
                      ymin = min(ranks_mint$rankings$er) + 5,
                      ymax = min(ranks_mint$rankings$er) + 150), ncol = 2)
```

### Accounting for ordinal outcomes in the project funding evaluation

Similar results are found when using a Bayesian hierarchical model that explicitly accounts for the ordinal nature of the scores, see Figure \@ref(fig:ordinal_mint). Figure \@ref(fig:agreement_ordinal_normal_mint) shows the Bland-Altman plot; the agreement between the BR with an ordinal outcome model and the BR with a continuous outcome model. The mean of all the differences (gray horizontal line) is very close to 0. As for the Postdoc.Mobility Panel, the differences in ER for most proposals lie in the 95\% limits of agreement (red dotted horizontal lines). However, even though both models result in similar results, it is hard to say which modelling approach performs better without knowing exactly which proposals should have been funded or rejected. We therefore compare the performance of the two models in a simulation study, where the true ranks are known.}

```{r mint-er-distr-plot-all-ordinal, echo = FALSE, fig.cap = "<span style='color: #d3d3d3;'>The ER - computed using an ordinal outcome model - ordered from the best (rank 1) to the worst (rank 352) together with their 50% credible intervals. A naive funding line to ensure a 30% success rate is drawn (blue dashed line).</span>", fig.height = 6, fig.width = 7, warning = FALSE, fig.align = "center"}

plot_er_cred_mint_ordinal <- 
  plot_er_distributions(get_mcmc_samples_result = mcmc_mint_ordinal,
                        n_proposals = mint_sections %>% 
                          summarise(n_distinct(proposal)) %>% pull(),
                        name_er_or_theta = "rank_theta", title = "(B)",
                        names_proposals = mint_sections %>% 
                          pull(proposal) %>% unique(),
                        number_fundable = x,
                        size_pt = .1, size_outer = 0, size_inner = .3,
                        alpha_inner = .8, outer_show = FALSE) +
  theme(axis.text.x = element_blank(),
        panel.grid.major.x = element_blank(),
        axis.title.y = element_text(size = 7, vjust = 2))

p.zoomrast_ordinal <- plot_er_cred_mint_ordinal + 
  xlim(levels(plot_er_cred_mint_ordinal$data$parameter)[90:121]) +
  ylim(c(74, 140)) + 
  zoomtheme

plot_er_cred_mint_ordinal + 
  theme(legend.position = "none") +
  annotation_custom(grob = ggplotGrob(p.zoomrast),
                    xmin = 160, xmax = 360, 
                    ymin = min(ranks_mint$rankings$er) + 5,
                    ymax = min(ranks_mint$rankings$er) + 150)
```


```{r display-agreement-plots-MINT, fig.align='center', fig.height = 6, fig.width = 9, warning=FALSE, message=FALSE, fig.cap = "<span style='color: #d3d3d3;'>Difference in expected rank depending on whether the outcome is modeled as continuous or ordinal variable versus the average ER as computed using the continuous and ordinal outcome model, for all proposals evaluated for project funding. The 95\\% limits of agreement are plotted (red dotted lines) as well as the mean difference (gray line, \\textit{i.e.} bias).</span>\\label{fig:agreement_ordinal_normal_mint}"}

get_agreement_plot(ranks_mint$rankings,
                   ranks_mint_ordinal$rankings,
                   title = "MINT - Project Funding",
                   ymin = -15, ymax = 15, pt_alpha = .5, 
                   pt_size = .8)

```


# Note on convergence diagnostics

The more complex the models the more burnin and iterations are needed to ensure convergence of the most important parameters. With the option  `dont_bind` set to `TRUE` in the `ERforResearch::get_mcmc_samples()` function, we can directly use the MCMC sample to perform convergence diagnostics, like plotting traceplots. The option in the function solely does not bind the different chains as for example the two chains used in our examples. Using he `bayesplot` package traceplots and others can easily be plotted. The following code chunk for example shows the traceplots of the $\theta_i$'s of all the proposals of the Biology Panel in Postdoc.mobility.

```{r traceplot, fig.cap = "<span style='color: #d3d3d3;'>Traceplots of the proposal effects of all 18 proposals in the Biology panel for the Postdoc.Mobility fellowships.</span>", fig.height = 6, fig.width = 9, warning = FALSE, fig.align = "center"}
bayesplot::mcmc_trace(mcmc_ls_b_object,
                      pars = paste0("application_intercept[", 1:18, "]"))
```


\textcolor{purple}{
Below we show the values $\hat{R}$ of the Gelman-Rubin convergence diagnostic \citep{Gelman1992}
and traceplots for some important parameters for the Social Sciences panel in the Postdoc.Mobility case study.
$\hat{R}$ values substantially above 1 indicate lack of convergence.
\citet{Gelman2014} have recommended to use $1.1$ as threshold value for $\hat{R}$.
<!-- \todo{Do not show the upper CI limits in the table?}} -->
\todo{for MO: add some traceplots}


```{r, rhat-values-computation}

## PM case study setting
n.iter <- 10000
n.burnin <- 5000
n.chains <- 2
seed <- 1991
seed_ordinal <- seed + 1

## Ordinal models:
# refit the model without pooling the 2 chains
mcmc_hss_s_ordinal_2 <-
  get_mcmc_samples(data = hss_s,
                   id_application = "proposal",
                   id_voter = "voter",
                   grade_variable = "num_grade",
                   n_chains = n.chains, n_iter = n.iter,
                   n_burnin = n.burnin,
                   other_variables = "nu",
                   ordinal_scale = TRUE, point_scale = 6,
                   path_to_jags_model =
                     here( "revision", "manuscript",
                           "jags_model_ordinal_interval.txt"),
                   seed = seed_ordinal, quiet = TRUE,
                   dont_bind = TRUE)

# do not perform multivariate computation since this sometimes leads to
# error messages and we do not need that result
# use the whole series, not only the second half (autoburnin = FALSE)
rhat_hss_s <- gelman.diag(mcmc_hss_s_ordinal_2, autoburnin = FALSE,
                         multivariate = FALSE)
# rhat_est_hss_s <- rhat_hss_s$psrf[ ,1]

# mcmc_hss_h_ordinal_2 <-
#   get_mcmc_samples(data = hss_h,
#                    id_application = "proposal",
#                    id_voter = "voter",
#                    grade_variable = "num_grade",
#                    n_chains = n.chains, n_iter = n.iter,
#                    n_burnin = n.burnin,
#                    other_variables = "nu",
#                    ordinal_scale = TRUE, point_scale = 6,
#                    path_to_jags_model =
#                      here( "revision", "manuscript",
#                            "jags_model_ordinal_interval.txt"),
#                    seed = seed_ordinal, quiet = TRUE,
#                    dont_bind = TRUE)
# 
# rhat_hss_h <- gelman.diag(mcmc_hss_h_ordinal_2, autoburnin = FALSE,
#                          multivariate = FALSE)

##########################################################

## Normal models:
# mcmc_hss_h_2 <-
#  get_mcmc_samples(data = hss_h,
#                    id_application = "proposal",
#                    id_voter = "voter",
#                    grade_variable = "num_grade",
#                    n_chains = n.chains, n_iter = n.iter,
#                    n_burnin = n.burnin,
#                    other_variables = "nu",
#                    seed = seed, quiet = TRUE,
#                   dont_bind = TRUE)
# 
# rhat_hss_h <- gelman.diag(mcmc_hss_h_2, autoburnin = FALSE,
#                          multivariate = FALSE)
# rhat_hss_h$psrf

# mcmc_ls_b_2 <-
#  get_mcmc_samples(data = ls_b,
#                    id_application = "proposal",
#                    id_voter = "voter",
#                    grade_variable = "num_grade",
#                    n_chains = n.chains, n_iter = n.iter,
#                    n_burnin = n.burnin,
#                    other_variables = "nu",
#                    seed = seed, quiet = TRUE,
#                   dont_bind = TRUE)
# 
# rhat_ls_b <- gelman.diag(mcmc_ls_b_2, autoburnin = FALSE,
#                          multivariate = FALSE)

# mcmc_ls_m_2 <-
#  get_mcmc_samples(data = ls_m,
#                    id_application = "proposal",
#                    id_voter = "voter",
#                    grade_variable = "num_grade",
#                    n_chains = n.chains, n_iter = n.iter,
#                    n_burnin = n.burnin,
#                    other_variables = "nu",
#                    seed = seed, quiet = TRUE,
#                   dont_bind = TRUE)
# 
# rhat_ls_m <- gelman.diag(mcmc_ls_m_2, autoburnin = FALSE,
#                          multivariate = FALSE)
# rhat_ls_m$psrf

## PF MINT setting:
# n.iter <- 40000
# n.burnin <- 20000
# n.chains <- 2
# seed <- 1991
# mcmc_mint <-
#   ERforResearch:::get_mcmc_samples(data = mint_sections,
#                                    id_application = "proposal",
#                                    id_voter = "voter",
#                                    id_section =  "section",
#                                    grade_variable = "num_grade",
#                                    tau_section_name = "tau_section",
#                                    n_chains = n.chains, n_iter = n.iter,
#                                    n_burnin = n.burnin,
#                                    path_to_jags_model =
#                                      here("revision", "manuscript",
#                                           "jags_model_with_sections.txt"),
#                                    seed = seed, quiet = TRUE,
#                                    dont_bind = TRUE)
# 
# rhat_mint <- gelman.diag(mcmc_mint, autoburnin = FALSE,
# some large R_hat values!


```

```{r, rhat-values-table}
# create R_hat values table for Social Sciences panel in PM 
n_proposal <- 18
n_voter <- 14
# adapt the row names to notation in the paper
rownames <- c(paste0("$\\theta_{", seq(1, n_proposal), "}$"),
              paste0("$\\nu_{", seq(1, n_voter), "}$"),
              paste0("rank", "$(\\theta_{", seq(1, n_proposal), "})$"),
              "$\\sigma$", "$\\tau$", "$\\tau_{\\nu}$")

table_hss_s <- as.data.frame(cbind(rhat_hss_s$psrf[ ,1]))#, rhat_hss_s$psrf[ ,2]))
row.names(table_hss_s) <- rownames

kable(table_hss_s, digits = 2, "latex", escape = FALSE,
      col.names =  c("$\\hat{R}$"),#, "Upper CI limit"),
      row.names = TRUE, booktabs = TRUE,
      longtable = TRUE,
      caption = "Estimates of $\\hat{R}$, the Gelman-Rubin convergence diagnostic are given for different parameters in the ordinal model for the Social Sciences panel in the PM case study.") %>%
  #, and the upper limits of the 95\\% confidence intervals for $\\hat{R}$
  kable_styling(latex_options = c("hold_position", "repeat_header"))
```
